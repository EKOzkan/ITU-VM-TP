{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uygws1WPElcO",
        "outputId": "a7ece29a-c112-4d58-aa4e-bced5ccedc28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-3c8e05adad33>:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\"/content/drive/MyDrive/bert_model/best_model.pt\",\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch.nn as nn\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the model class\n",
        "class BERTClass(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.bert_model = BertModel.from_pretrained('bert-base-uncased')  # Changed from self.bert to self.bert_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.linear = nn.Linear(768, 32)  # Changed to 32 labels based on the context\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output = self.bert_model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "        dropout_output = self.dropout(output[1])\n",
        "        final_output = self.linear(dropout_output)\n",
        "        return final_output\n",
        "\n",
        "# Define target labels (32 labels based on the context)\n",
        "target_list = ['ad hominem',\n",
        " 'anecdotal fallacy',\n",
        " 'appeal to authority',\n",
        " 'appeal to consequences',\n",
        " 'appeal to emotion',\n",
        " 'appeal to fear',\n",
        " 'appeal to novelty',\n",
        " 'appeal to popularity',\n",
        " 'appeal to ridicule',\n",
        " 'appeal to tradition',\n",
        " 'argument from ignorance',\n",
        " 'bandwagon fallacy',\n",
        " 'circular reasoning',\n",
        " 'correlation vs. causation',\n",
        " 'equivocation',\n",
        " 'false analogy',\n",
        " 'false attribution',\n",
        " 'false dilemma',\n",
        " 'genetic fallacy',\n",
        " 'guilt by association',\n",
        " 'hasty generalization',\n",
        " 'no true scotsman',\n",
        " 'red herring',\n",
        " 'slippery slope',\n",
        " 'straw man',\n",
        " 'tu quoque',\n",
        " 'appeal to motive',\n",
        " 'loaded question',\n",
        " 'misleading vividness',\n",
        " 'none',\n",
        " 'composition/division',\n",
        " 'other']\n",
        "\n",
        "def load_model():\n",
        "    # Initialize tokenizer\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # Initialize model\n",
        "    model = BERTClass()\n",
        "\n",
        "    # Load the trained model weights\n",
        "    checkpoint = torch.load(\"/content/drive/MyDrive/bert_model/best_model.pt\",\n",
        "                          map_location=device) #download and use the model weights from: https://drive.google.com/file/d/1-8nspRmZ0x6pMZrOdWgPV9RINjFZYm0B/view?usp=share_link\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def predict_text(raw_text, model, tokenizer):\n",
        "    # Tokenize the input text\n",
        "    encoded_text = tokenizer.encode_plus(\n",
        "        raw_text,\n",
        "        max_length=256,\n",
        "        add_special_tokens=True,\n",
        "        return_token_type_ids=True,\n",
        "        padding='max_length',\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt',\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Prepare input tensors\n",
        "    input_ids = encoded_text['input_ids'].to(device, dtype=torch.long)\n",
        "    attention_mask = encoded_text['attention_mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = encoded_text['token_type_ids'].to(device, dtype=torch.long)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        predictions = (outputs > 0.5).float()  # Using 0.5 as threshold\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nAnalyzing: {raw_text}\")\n",
        "    print(\"\\nDetected fallacies:\")\n",
        "    found_fallacy = False\n",
        "    for idx, p in enumerate(predictions[0]):\n",
        "        if p == 1:\n",
        "            found_fallacy = True\n",
        "            print(f\"- {target_list[idx]}\")\n",
        "\n",
        "    if not found_fallacy:\n",
        "        print(\"No fallacies detected.\")\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Loading model...\")\n",
        "    model, tokenizer = load_model()\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYNdO_W2GDjw",
        "outputId": "dd2c6d69-0c26-443c-c1f2-0d54fe446e62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing: I saw a red apple so all the apples are red\n",
            "\n",
            "Detected fallacies:\n",
            "- hasty generalization\n"
          ]
        }
      ],
      "source": [
        "text = \"I saw a red apple so all the apples are red\"\n",
        "\n",
        "predict_text(text, model, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zY6bt4AwKarm",
        "outputId": "41c2b3e9-81a9-41ee-d092-922428e96384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Analyzing: I like your shoes.\n",
            "\n",
            "Detected fallacies:\n",
            "No fallacies detected.\n"
          ]
        }
      ],
      "source": [
        "text = \"I like your shoes.\"\n",
        "\n",
        "predict_text(text, model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QS-l0wdWLJe3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
